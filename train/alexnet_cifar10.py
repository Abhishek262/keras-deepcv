"""
Train an AlexNet model with the CIFAR10 dataset

Print model:
	python alexnet_cifar10.py --print_model

Train and save model:
	python alexnet_cifar10.py --train_model --epochs 10 \
		--save_weights data/alexnet_mnist_trained.hdf5

Train with pretrained weights
	python alexnet_mnist.py --train_model --epochs 10 \
		--save_weights data/alexnet_mnist_trained.hdf5 \
		--weights data/alexnet_mnist.hdf5
"""
import sys
sys.path.append("..")

# Import model architecture and data
from models.classification import alexnet
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import rmsprop
from datasets import cifar10

# Import other necessary packages
import numpy as np
import argparse, cv2, os, pickle
import matplotlib.pyplot as plt

def draw_training_curve(history):
	"""
	Draw training curve

	Parameters:
		history - contains loss and accuracy from training
	Returns:
		None
	"""
	plt.figure(1)

	# History for accuracy
	plt.subplot(211)
	plt.plot(history.history['acc'])
	plt.plot(history.history['val_acc'])
	plt.title('model accuracy')
	plt.ylabel('accuracy')
	plt.xlabel('epoch')
	plt.legend(['train', 'test'], loc='upper left')

	# History for loss
	plt.subplot(212)
	plt.plot(history.history['loss'])
	plt.plot(history.history['val_loss'])
	plt.title('model loss')
	plt.ylabel('loss')
	plt.xlabel('epoch')
	plt.legend(['train', 'test'], loc='upper left')

	plt.show()

def parse_args():
	"""
	Parse command line arguments.

	Parameters:
		None
	Returns:
		parser arguments
	"""
	parser = argparse.ArgumentParser(description='LeNet model')
	optional = parser._action_groups.pop()
	required = parser.add_argument_group('required arguments')
	optional.add_argument('--print_model',
		dest='print_model',
		help='Print LeNet model',
		action='store_true')
	optional.add_argument('--train_model',
		dest='train_model',
		help='Train LeNet on MNIST',
		action='store_true')
	optional.add_argument('-s', '--save_weights',
		dest='save_weights',
		help='Save the trained weights',
		default=None)
	optional.add_argument('-w', '--weights',
		dest='weights',
		help='Path to weights (hdf5) file',
		default=None)
	optional.add_argument('-e', '--epochs',
		dest='epochs',
		help='Number of epochs for training',
		type=int,
		default=20)
	optional.add_argument('--data_augmentation',
		dest='data_augmentation',
		help='Use data augmentations for input',
		action='store_true')
	parser._action_groups.append(optional)
	return parser.parse_args()

if __name__ == '__main__':
	# Command line parameters
	args = parse_args()

	# Construct AlexNet model
	if args.weights is None:
		model = alexnet.alexnet_bn_model(img_shape=(32, 32, 3))
	else:
		model = alexnet.alexnet_bn_model(img_shape=(32, 32, 3),
					weights=args.weights)

	# Print model summary
	if args.print_model:
		model.summary()

	# Compile model
	model.compile(loss='categorical_crossentropy',
		optimizer=rmsprop(lr=0.0001, decay=1e-6),
		metrics=['accuracy'])

	# Get CIFAR10 data
	train_data, train_labels, test_data, test_labels = cifar10.get_data()

	# Train with no data augmentation
	if not args.data_augmentation:
		print('[INFO] Training the model (without data augmentation)...')
		history = model.fit(train_data, train_labels,
			batch_size=128,
			epochs=args.epochs,
			validation_data=(test_data, test_labels),
			verbose=True,
			shuffle=True)

		# Evaluate the model
		print('[INFO] Evaluating the trained model...')
		(loss, accuracy) = model.evaluate(test_data, test_labels,
			batch_size=128,
			verbose=1)
		print('[INFO] accuracy: {:.2f}%'.format(accuracy * 100))

		# Visualize training history
		draw_training_curve(history)

	# Train with data augmentation
	else:
		print('[INFO] Training the model with data augmentation...')
		# Data augmentation
		datagen = ImageDataGenerator(
			featurewise_center=False,	# set input mean to 0 over dataset
			samplewise_center=False,	# set each sample mean to 0
			featurewise_std_normalization=False,	# divide inputs by std of dataset
			samplewise_std_normalization=False,	#divide each input by its std
			zca_whitening=False,	# apply ZCA whitening
			rotation_range=0,	# randomly roate images in the range (degrees, 0 to 180)
			width_shift_range=0.1,	# randomly shift image horizontally (fraction of width)
			height_shift_range=0.1,	# randomly shift image vertically (fraction of height)
			horizontal_flip=True,	# randomly flip images horizontally
			vertical_flip=False)	# randomly flip images vertically

		# Compute std, mean, and principal components if ZCA is applied
		datagen.fit(train_data)

		# Train the data with batches generated by datagen.flow()
		model.fit_generator(datagen.flow(train_data, train_labels,
			batch_size=128),
			steps_per_epoch=train_data.shape[0] // 128,
			epochs=args.epochs,
			validation_data=(test_data, test_labels),
			workers=4)

		# Evaluate the model
		print('[INFO] Evalauting the trained model...')
		(loss, accuracy) = model.evaluate_generator(
			datagen.flow(train_data, train_label,
				batch_size=128,
				shuffle=False),
			steps=test_data.shape[0] // 128,
			workers=4)
		print('[INFO] accuracy: {:.2f}%'.format(accuracy * 100))

	# Save model and weights
	if args.save_weights is not None:
		print('[INFO] Saving the model weights to file...')
		model.save_weights(args.save_weights, overwrite=True)